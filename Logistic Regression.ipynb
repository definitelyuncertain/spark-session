{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math , sys\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# hyper-parameters and configuration options \n",
    "NUM_ITERATIONS = 100\n",
    "LEARNING_RATE = 0.3\n",
    "SHOW_PROGRESS = True\n",
    "DECISION_BOUNDARY_STYLE = ':'\n",
    "\n",
    "\n",
    "def sigmoid(gamma):\n",
    "  if gamma < 0:\n",
    "    return 1 - 1 / (1 + math.exp(gamma))\n",
    "  return 1 / (1 + math.exp(-gamma))\n",
    "\n",
    "def plot_decision_boundary(w):\n",
    "  x = np.linspace(1, 10, 50)\n",
    "  y = -(w[0]*x + w[2])/w[1]\n",
    "  plt.plot(x, y, DECISION_BOUNDARY_STYLE)\n",
    "\n",
    "\n",
    "# generate artificial datasets for binary classification\n",
    "class_a = np.random.multivariate_normal([3, 3], [[1, 0], [0, 1]], 100)\n",
    "class_b = np.random.multivariate_normal([8, 8], [[1, 0], [0, 1]], 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the dataset for logistic regression  and distribute it \n",
    "part_a = sc.parallelize(class_a).map(lambda features: LabeledPoint(1, np.append(features, [1])))\n",
    "part_b = sc.parallelize(class_b).map(lambda features: LabeledPoint(-1, np.append(features, [1])))\n",
    "training_data = part_a.union(part_b).cache()\n",
    "\n",
    "\n",
    "# initialize the parameters \n",
    "w = np.random.randn(1, 3)\n",
    "w = w[0,:]\n",
    "\n",
    "# visualize the dataset\n",
    "plt.plot(class_a[:,0], class_a[:,1], 'bo')\n",
    "plt.plot(class_b[:,0], class_b[:,1], 'ro')\n",
    "plot_decision_boundary(w)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset you saw upon running the cells above has 2 classes, and you will implement Logistic Regression with stochastic gradient descent to perform classification. The steps are as follows:\n",
    "\n",
    "* The data has been put into an RDD `training_data` for you. This has labels and features you can access as `x.label` and `x.features` for an element `x` which is being referred to in the map phase.\n",
    "* The weights are available as `w` and are initialized to the values corresponding to the line you see in the above plot.\n",
    "* The _map_ phase is to compute the gradient using the usual logistic regression update. This is found as\n",
    "$$ (\\sigma (y \\cdot w^{T}x) - 1)\\cdot y \\cdot x $$\n",
    "  where $\\sigma$ is the logistic function defined for you as `sigmoid`, $x$ is the feature vector and $y$ is the label which is given as +1 or -1.\n",
    "* Next, randomly sample elements using the `gradient.sample(False, 0.05)`. This will return an RDD with the gradient of a random set of 5 percent of the data.\n",
    "* The _reduce_ phase is just to sum up these gradients as before.\n",
    "* Once you have the minibatch gradient, update the weights with the learning rate as defined for you in the variable `LEARNING_RATE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train logistic regression with distributed gradient descent\n",
    "for i in range(1, NUM_ITERATIONS+1):\n",
    "  sys.stdout.write( \"EPOCH %2d :\"% (i))\n",
    "\n",
    "  # Your code to do the gradient update\n",
    "  \n",
    "  ###########\n",
    "\n",
    "  if SHOW_PROGRESS:\n",
    "    #print the line equation \n",
    "    #print  str(format(w[0], '3.2f')) + ' x + ' + str(round(w[1],2)) + ' y + ' + str(round(w[2],2))\n",
    "    print( \" %7.2f x + %7.2f y + %7.2f \" % (w[0], w[1], w[2]))\n",
    "    plot_decision_boundary(w)\n",
    "\n",
    "# plot the final decision boundary \n",
    "x = np.linspace(1, 12, 20)\n",
    "y = -(w[0]*x + w[2])/w[1]\n",
    "plt.plot(x, y,'g', linewidth=2)\n",
    "\n",
    "# plot the dataset \n",
    "plt.plot(class_a[:,0], class_a[:,1], 'bo')\n",
    "plt.plot(class_b[:,0], class_b[:,1], 'ro')\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([-4, 12])\n",
    "\n",
    "# show the evolution of decision boundary along with the dataset\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
